function [J, grad] = costFunctionReg(theta, X, y, lambda)           % Compute cost and gradient for logistic regression with regularization
m = length(y); % number of training examples
% You need to return the following variables correctly 
g=sigmoid(X*theta);
J = 0;
grad = zeros(size(theta));
J = costFunction(theta, X, y) + (lambda/(2*m))*(sum(theta(2:length(theta)).^2));    %cost for logistic regression with regularization
grad=[(X')*(g-y) + lambda*theta]/m;              %gradient for logistic regression with regularization
grad(1)=grad(1)-lambda*theta(1)/m;
end
