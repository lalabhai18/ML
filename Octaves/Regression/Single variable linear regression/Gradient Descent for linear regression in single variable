function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)  
theta1=theta';
m = length(y);      % number of training examples
J_history = zeros(num_iters, 1);
for iter = 1:num_iters
  theta=theta1';
  theta1=theta1-((X*(theta)-y)'*X)*(alpha/m);    %simultaneous update of theta vector
  J_history(iter) = computeCost(X, y, theta);    %cost value corresponding to each theta value
endfor
end
